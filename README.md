# Audio Style Transfer of Accents in Human Speech
---
Dante Amicarella, Ivan Klevanski, Emre Sabaz
Worcester Polytechnic Institute


## Abstract
This project explores the application of audio style transfer (AST) for accent modification in human speech. By adapting and extending existing text-to-speech (TTS) architectures, we introduce a pipeline that leverages neural style transfer (NST) and a speaker-conditional WaveRNN (SC-WaveRNN) vocoder to achieve accent conversion. The system integrates pre-trained VGG19-based NST for capturing accent features and combines them with content representations generated by Tacotron. Speaker embeddings, derived using generalized end-to-end (GE2E) loss, enable zero-shot adaptation to unseen speakers. Experiments demonstrate the system's ability to retain linguistic content while incorporating distinct accent features, validated through mel-spectrogram analysis and synthesized audio quality. Despite challenges in data handling, resource constraints, and model modifications, the results highlight the pipeline's potential for seamless accent transfer. Future work aims to improve the system's robustness and explore advanced neural architectures for enhanced performance.

---

## Model Architecture
Below is the architecture diagram illustrating the pipeline used for accent transfer:

![Screenshot 2024-12-11 at 10 00 34 PM](https://github.com/user-attachments/assets/3139b150-3569-433e-8818-54cdb8918f88)

---

## File Structure
The repository is organized as follows:
├── _misc/                   # Miscellaneous files
├── _unused/                 # Unused files
├── executables/             # Executable scripts for running specific tasks
├── model_weights/           # Fine-tuned and resulting model weights
├── models/                  # Model architecture (accent encoder, SC-WaveRNN, speaker encoder, Tacotron)
├── utils/                   # Helper functions
├── datasets.py              # Script for dataset processing and loading
├── hparams.py               # Hyperparameters (read in VCTK and Libri data here)
---

## References
1. Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). A Neural Algorithm of Artistic Style. *arXiv preprint arXiv:1508.06576*. [Paper Link](https://arxiv.org/abs/1508.06576)
2. Wang, Y., et al. (2017). Tacotron: Towards End-to-End Speech Synthesis. *arXiv preprint arXiv:1703.10135*. [Paper Link](https://arxiv.org/abs/1703.10135)
3. Wan, L., et al. (2018). Generalized End-to-End Loss for Speaker Verification. *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. [Paper Link](https://arxiv.org/abs/1710.10467)
4. Deshpande, A., et al. (2019). An Audio Style Transfer Model Based on Timbre and Rhythm. *arXiv preprint arXiv:1906.03612*. [Paper Link](https://arxiv.org/abs/1906.03612)
5. Paul, D., et al. (2020). Speaker Conditional WaveRNN: Towards Universal Text-to-Speech. *arXiv preprint arXiv:2005.09100*. [Paper Link](https://arxiv.org/abs/2005.09100)

---

## Acknowledgments
This project builds on the foundation provided by the [SC-WaveRNN repository](https://github.com/dipjyoti92/SC-WaveRNN/tree/master). We thank the authors for their contributions to the field.


